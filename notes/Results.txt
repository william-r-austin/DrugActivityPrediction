K values for feature reduction using chi-squared and a bernoulli NB model. Alpha = 0.01, Fit Prior = True
k = 5, avg = 0.6400091136933241
k = 15, avg = 0.6545398410386969
k = 25, avg = 0.6688314990374487
k = 35, avg = 0.6652083106316516
k = 45, avg = 0.6783612522742957
k = 55, avg = 0.6883863149309374
k = 65, avg = 0.6760179434092477
k = 75, avg = 0.6893787027327399
k = 85, avg = 0.6857555143269429
k = 95, avg = 0.6857555143269429
k = 105, avg = 0.6989587605377079
k = 115, avg = 0.6982784884288644
k = 125, avg = 0.7118839306057352
k = 135, avg = 0.7166458353676399
k = 145, avg = 0.7166458353676399
k = 155, avg = 0.7166458353676399
k = 165, avg = 0.7184204582602752
k = 175, avg = 0.732910523060899
k = 185, avg = 0.732910523060899
k = 195, avg = 0.7354985147793256
k = 205, avg = 0.7354985147793256
k = 215, avg = 0.7421651814459923
k = 225, avg = 0.7476167691957166
k = 235, avg = 0.740852374170452
k = 245, avg = 0.7483138567872141
k = 255, avg = 0.7442759071592023
k = 265, avg = 0.7442759071592023
k = 275, avg = 0.7401972322109623
k = 285, avg = 0.7577846790890269
k = 295, avg = 0.7673837756446452
k = 305, avg = 0.759113098952916
k = 315, avg = 0.7647596263385738
k = 325, avg = 0.7647596263385738
k = 335, avg = 0.7730303030303031
k = 345, avg = 0.7730303030303031
k = 355, avg = 0.7730303030303031
k = 365, avg = 0.7643368019144418
k = 375, avg = 0.7590987066763465
k = 385, avg = 0.7542635418411816
k = 395, avg = 0.7542635418411816
k = 405, avg = 0.7486976791324619
k = 415, avg = 0.7486976791324619
k = 425, avg = 0.7486976791324619
k = 435, avg = 0.7486976791324619
k = 445, avg = 0.7486976791324619
k = 455, avg = 0.7486976791324619
k = 465, avg = 0.7486976791324619
k = 475, avg = 0.7327875023527197
k = 485, avg = 0.7327875023527197
k = 495, avg = 0.7421042725390553
k = 505, avg = 0.7327875023527197
k = 515, avg = 0.7327875023527197
k = 525, avg = 0.7404065499717672
k = 535, avg = 0.7378185582533409
k = 545, avg = 0.7331601731601731
k = 555, avg = 0.7225085440874915
k = 565, avg = 0.7225085440874915
k = 575, avg = 0.7225085440874915
k = 585, avg = 0.7271669291806592
k = 595, avg = 0.7225085440874915
k = 605, avg = 0.7339371155160629
k = 615, avg = 0.7248894964684439
k = 625, avg = 0.7182228298017772
k = 635, avg = 0.7182228298017772
k = 645, avg = 0.7182228298017772
k = 655, avg = 0.7270140385929861
k = 665, avg = 0.7351540467329941
k = 675, avg = 0.7351540467329941
k = 685, avg = 0.7351540467329941
k = 695, avg = 0.7351540467329941
k = 705, avg = 0.7351540467329941
k = 715, avg = 0.7351540467329941
k = 725, avg = 0.7351540467329941
k = 735, avg = 0.7351540467329941
k = 745, avg = 0.7394397610187085
k = 755, avg = 0.7394397610187085
k = 765, avg = 0.7394397610187085
k = 775, avg = 0.7422887638677114
k = 785, avg = 0.7422887638677114
k = 795, avg = 0.7453656869446343
k = 805, avg = 0.7453656869446343
k = 815, avg = 0.7453656869446343
k = 825, avg = 0.7453656869446343
k = 835, avg = 0.7532708252845552
k = 845, avg = 0.7532708252845552
k = 855, avg = 0.7605172020961496
k = 865, avg = 0.7605172020961496
k = 875, avg = 0.7605172020961496
k = 885, avg = 0.7651755871893172
k = 895, avg = 0.7651755871893172
k = 905, avg = 0.7651755871893172
k = 915, avg = 0.7651755871893172
k = 925, avg = 0.7651755871893172
k = 935, avg = 0.7703515706261702
k = 945, avg = 0.7703515706261702
k = 955, avg = 0.7703515706261702
k = 965, avg = 0.7611619298575821
k = 975, avg = 0.7611619298575821
k = 985, avg = 0.7611619298575821
k = 995, avg = 0.7611619298575821

Max is k = 345, avg = 0.7730303030303031
==============================================================================================================
K values for feature reduction using chi-squared and a bernoulli NB model. Alpha = 0.01, Fit Prior = FALSE

k = 5, avg = 0.685870110057753
k = 15, avg = 0.68699180663875
k = 25, avg = 0.6966391133112284
k = 35, avg = 0.6749827467218772
k = 45, avg = 0.6749827467218772
k = 55, avg = 0.6831231570362004
k = 65, avg = 0.6812382924184167
k = 75, avg = 0.6848614808242137
k = 85, avg = 0.6857555143269429
k = 95, avg = 0.6901832503336263
k = 105, avg = 0.6935165836669597
k = 115, avg = 0.6935165836669597
k = 125, avg = 0.7015561631351106
k = 135, avg = 0.7012361932499233
k = 145, avg = 0.7028015843805318
k = 155, avg = 0.7166458353676399
k = 165, avg = 0.7184204582602752
k = 175, avg = 0.7272639956752412
k = 185, avg = 0.7272639956752412
k = 195, avg = 0.7272639956752412
k = 205, avg = 0.7476167691957166
k = 215, avg = 0.7476167691957166
k = 225, avg = 0.7368883671629668
k = 235, avg = 0.7473120014996445
k = 245, avg = 0.7416654741139866
k = 255, avg = 0.7394407423240376
k = 265, avg = 0.7347823572308698
k = 275, avg = 0.7347823572308698
k = 285, avg = 0.7477114190157668
k = 295, avg = 0.7525465838509316
k = 305, avg = 0.7525465838509316
k = 315, avg = 0.7525465838509316
k = 325, avg = 0.762866553736119
k = 335, avg = 0.7673837756446452
k = 345, avg = 0.7673837756446452
k = 355, avg = 0.7673837756446452
k = 365, avg = 0.7627103331451158
k = 375, avg = 0.7578751683099509
k = 385, avg = 0.7578751683099509
k = 395, avg = 0.7616846921194748
k = 405, avg = 0.7486976791324619
k = 415, avg = 0.7486976791324619
k = 425, avg = 0.7486976791324619
k = 435, avg = 0.7486976791324619
k = 445, avg = 0.7486976791324619
k = 455, avg = 0.7486976791324619
k = 465, avg = 0.7486976791324619
k = 475, avg = 0.7486976791324619
k = 485, avg = 0.7486976791324619
k = 495, avg = 0.7486976791324619
k = 505, avg = 0.7506757011104838
k = 515, avg = 0.7497233201581028
k = 525, avg = 0.7497233201581028
k = 535, avg = 0.7468661773009598
k = 545, avg = 0.7331601731601731
k = 555, avg = 0.7307792207792208
k = 565, avg = 0.7307792207792208
k = 575, avg = 0.7307792207792208
k = 585, avg = 0.7307792207792208
k = 595, avg = 0.7264935064935064
k = 605, avg = 0.7270140385929861
k = 615, avg = 0.7270140385929861
k = 625, avg = 0.7270140385929861
k = 635, avg = 0.7270140385929861
k = 645, avg = 0.7270140385929861
k = 655, avg = 0.7270140385929861
k = 665, avg = 0.7351540467329941
k = 675, avg = 0.7351540467329941
k = 685, avg = 0.7351540467329941
k = 695, avg = 0.7351540467329941
k = 705, avg = 0.7427126257201445
k = 715, avg = 0.7427126257201445
k = 725, avg = 0.7427126257201445
k = 735, avg = 0.7351540467329941
k = 745, avg = 0.7351540467329941
k = 755, avg = 0.7351540467329941
k = 765, avg = 0.7351540467329941
k = 775, avg = 0.7351540467329941
k = 785, avg = 0.7351540467329941
k = 795, avg = 0.7394397610187085
k = 805, avg = 0.7394397610187085
k = 815, avg = 0.7394397610187085
k = 825, avg = 0.7473448993586294
k = 835, avg = 0.7501939022076323
k = 845, avg = 0.7532708252845552
k = 855, avg = 0.7605172020961496
k = 865, avg = 0.7605172020961496
k = 875, avg = 0.7605172020961496
k = 885, avg = 0.7605172020961496
k = 895, avg = 0.7605172020961496
k = 905, avg = 0.7605172020961496
k = 915, avg = 0.7605172020961496
k = 925, avg = 0.7605172020961496
k = 935, avg = 0.7651755871893172
k = 945, avg = 0.7651755871893172
k = 955, avg = 0.7651755871893172
k = 965, avg = 0.7651755871893172
k = 975, avg = 0.7651755871893172
k = 985, avg = 0.7703515706261702
k = 995, avg = 0.7703515706261702
Best value is k = 985, 0.7703515706261702

Same as above
k = 1025, avg = 0.7611619298575821
k = 1075, avg = 0.76907401776967
k = 1125, avg = 0.761159629420499
k = 1175, avg = 0.7581361978256388
k = 1225, avg = 0.7435797085632672
k = 1275, avg = 0.7435797085632672
k = 1325, avg = 0.7315650565486153
k = 1375, avg = 0.7315650565486153
k = 1425, avg = 0.7264831819015232
k = 1475, avg = 0.7213071984646702
k = 1525, avg = 0.719142696300168
k = 1575, avg = 0.7140608216530759
k = 1625, avg = 0.719142696300168
k = 1675, avg = 0.7140608216530759
k = 1725, avg = 0.7140608216530759
k = 1775, avg = 0.7065022426659254
k = 1825, avg = 0.7065022426659254
k = 1875, avg = 0.7065022426659254
k = 1925, avg = 0.7065022426659254
k = 1975, avg = 0.7115841173130175
k = 2025, avg = 0.7115841173130175
k = 2075, avg = 0.7034441091730095
k = 2125, avg = 0.6964111421400424
k = 2175, avg = 0.6964111421400424
k = 2225, avg = 0.6964111421400424
k = 2275, avg = 0.6925123494213737
k = 2325, avg = 0.6836688120064078
k = 2375, avg = 0.6836688120064078
k = 2425, avg = 0.6738943759161823
k = 2475, avg = 0.6738943759161823
k = 2525, avg = 0.6623397540674428
k = 2575, avg = 0.6802541982976765
k = 2625, avg = 0.6747654764931653
k = 2675, avg = 0.6792826984016915
k = 2725, avg = 0.6966157134790675
k = 2775, avg = 0.6821317012506943
k = 2825, avg = 0.69190613734092
k = 2875, avg = 0.6821317012506943
k = 2925, avg = 0.6821317012506943
k = 2975, avg = 0.6948772403120228
k = 3025, avg = 0.6871442325790152
k = 3075, avg = 0.6902211556559382
k = 3125, avg = 0.694879540749106
k = 3175, avg = 0.7128166913178354
k = 3225, avg = 0.6948772403120228
k = 3275, avg = 0.7095606880618321
k = 3325, avg = 0.7099841776157565
k = 3375, avg = 0.6996391450739277
k = 3425, avg = 0.6996391450739277
k = 3475, avg = 0.6832481286829113
k = 3525, avg = 0.6832481286829113
k = 3575, avg = 0.6962442943106558
k = 3625, avg = 0.692288250354612
k = 3675, avg = 0.6962442943106558
k = 3725, avg = 0.6909811364159191
k = 3775, avg = 0.6909811364159191
k = 3825, avg = 0.6909811364159191
k = 3875, avg = 0.6833620887968713
k = 3925, avg = 0.6833620887968713
k = 3975, avg = 0.6786001840349665
k = 4025, avg = 0.6822631876979702
k = 4075, avg = 0.6712910381543921
k = 4125, avg = 0.6596360264993805
k = 4175, avg = 0.6669620338253878
k = 4225, avg = 0.6669620338253878
k = 4275, avg = 0.6669620338253878
k = 4325, avg = 0.6669620338253878
k = 4375, avg = 0.6517308881594596
k = 4425, avg = 0.6596360264993805
k = 4475, avg = 0.6596360264993805
k = 4525, avg = 0.65568345732942
k = 4575, avg = 0.65568345732942
k = 4625, avg = 0.6477783189894991
k = 4675, avg = 0.6477783189894991
k = 4725, avg = 0.6477783189894991
k = 4775, avg = 0.6387438751724466
k = 4825, avg = 0.6477783189894991
k = 4875, avg = 0.6387438751724466
k = 4925, avg = 0.627929292929293
k = 4975, avg = 0.627929292929293
k = 5025, avg = 0.627929292929293
k = 5075, avg = 0.627929292929293
k = 5125, avg = 0.627929292929293
k = 5175, avg = 0.6369637367463454
k = 5225, avg = 0.6448688750862663
k = 5275, avg = 0.6369637367463454
k = 5325, avg = 0.6402197400023486
k = 5375, avg = 0.6402197400023486
k = 5425, avg = 0.6402197400023486
k = 5475, avg = 0.6402197400023486
k = 5525, avg = 0.6402197400023486
k = 5575, avg = 0.6402197400023486
k = 5625, avg = 0.6402197400023486
k = 5675, avg = 0.6402197400023486
k = 5725, avg = 0.6402197400023486
k = 5775, avg = 0.6402197400023486
k = 5825, avg = 0.6402197400023486
k = 5875, avg = 0.6402197400023486
k = 5925, avg = 0.6402197400023486
k = 5975, avg = 0.63156173134434
k = 6025, avg = 0.6355143005143005
k = 6075, avg = 0.6355143005143005
k = 6125, avg = 0.6355143005143005
k = 6175, avg = 0.6355143005143005
k = 6225, avg = 0.6355143005143005
k = 6275, avg = 0.6383633033633033
k = 6325, avg = 0.6383633033633033
k = 6375, avg = 0.6383633033633033
k = 6425, avg = 0.6383633033633033
k = 6475, avg = 0.6426923076923076
k = 6525, avg = 0.6426923076923076
k = 6575, avg = 0.6426923076923076
k = 6625, avg = 0.6426923076923076
k = 6675, avg = 0.6426923076923076
k = 6725, avg = 0.6426923076923076
k = 6775, avg = 0.6426923076923076
k = 6825, avg = 0.6426923076923076
k = 6875, avg = 0.6426923076923076
k = 6925, avg = 0.6426923076923076
k = 6975, avg = 0.6426923076923076
k = 7025, avg = 0.6426923076923076
k = 7075, avg = 0.6426923076923076
k = 7125, avg = 0.6426923076923076
k = 7175, avg = 0.6426923076923076
k = 7225, avg = 0.6426923076923076
k = 7275, avg = 0.6426923076923076
k = 7325, avg = 0.6426923076923076
k = 7375, avg = 0.6426923076923076
k = 7425, avg = 0.6426923076923076
k = 7475, avg = 0.6426923076923076
k = 7525, avg = 0.6426923076923076
k = 7575, avg = 0.6426923076923076
k = 7625, avg = 0.6426923076923076
k = 7675, avg = 0.6457692307692308
k = 7725, avg = 0.6457692307692308
k = 7775, avg = 0.6457692307692308
k = 7825, avg = 0.6457692307692308
k = 7875, avg = 0.6457692307692308
k = 7925, avg = 0.6362454212454213
k = 7975, avg = 0.6362454212454213
k = 8025, avg = 0.6362454212454213
k = 8075, avg = 0.6362454212454213
k = 8125, avg = 0.6276739926739926
k = 8175, avg = 0.6276739926739926
k = 8225, avg = 0.6324358974358973
k = 8275, avg = 0.6324358974358973
k = 8325, avg = 0.6324358974358973
k = 8375, avg = 0.6324358974358973
k = 8425, avg = 0.6324358974358973
k = 8475, avg = 0.6324358974358973
k = 8525, avg = 0.6229120879120879
k = 8575, avg = 0.6135953177257525
k = 8625, avg = 0.6229120879120879
k = 8675, avg = 0.6229120879120879
k = 8725, avg = 0.6135953177257525
k = 8775, avg = 0.6135953177257525
k = 8825, avg = 0.6135953177257525
k = 8875, avg = 0.6135953177257525
k = 8925, avg = 0.6178810320114668
k = 8975, avg = 0.6178810320114668
k = 9025, avg = 0.6178810320114668
k = 9075, avg = 0.6101170568561872
k = 9125, avg = 0.6101170568561872
k = 9175, avg = 0.6101170568561872
k = 9225, avg = 0.6101170568561872
k = 9275, avg = 0.6101170568561872
k = 9325, avg = 0.5999533075620033
k = 9375, avg = 0.5999533075620033
k = 9425, avg = 0.5999533075620033
k = 9475, avg = 0.5999533075620033
k = 9525, avg = 0.5999533075620033
k = 9575, avg = 0.6034697910784868
k = 9625, avg = 0.6034697910784868
k = 9675, avg = 0.6034697910784868
k = 9725, avg = 0.6034697910784868
k = 9775, avg = 0.6034697910784868
k = 9825, avg = 0.6034697910784868
k = 9875, avg = 0.6034697910784868
k = 9925, avg = 0.6034697910784868
k = 9975, avg = 0.6034697910784868
Best value is k = 1075, 0.76907401776967

================================================================================================
== My custom reducer (top-k). Folds = 7, Alpha = 0.0001
================================================================================================
My reducer. Feature Count = 5   Avg Score = 0.3516091051805338
My reducer. Feature Count = 10   Avg Score = 0.4725959614615077
My reducer. Feature Count = 15   Avg Score = 0.5884671817444926
My reducer. Feature Count = 20   Avg Score = 0.6383538030596855
My reducer. Feature Count = 25   Avg Score = 0.6698283226920998
My reducer. Feature Count = 30   Avg Score = 0.6800990976687571
My reducer. Feature Count = 35   Avg Score = 0.6800990976687571
My reducer. Feature Count = 40   Avg Score = 0.6883697743604865
My reducer. Feature Count = 45   Avg Score = 0.6995742561532036
My reducer. Feature Count = 50   Avg Score = 0.7168427019930779
My reducer. Feature Count = 55   Avg Score = 0.7168427019930779
My reducer. Feature Count = 60   Avg Score = 0.7508563074352548
My reducer. Feature Count = 65   Avg Score = 0.7508563074352548
My reducer. Feature Count = 70   Avg Score = 0.7639515455304929
My reducer. Feature Count = 75   Avg Score = 0.7639515455304929
My reducer. Feature Count = 80   Avg Score = 0.792859073453943
My reducer. Feature Count = 85   Avg Score = 0.8020487142225311
My reducer. Feature Count = 90   Avg Score = 0.8020487142225311
My reducer. Feature Count = 95   Avg Score = 0.8020487142225311
My reducer. Feature Count = 100   Avg Score = 0.8020487142225311
My reducer. Feature Count = 105   Avg Score = 0.8205901658909178
My reducer. Feature Count = 110   Avg Score = 0.8205901658909178
My reducer. Feature Count = 115   Avg Score = 0.8321447877396573
My reducer. Feature Count = 120   Avg Score = 0.8321447877396573
My reducer. Feature Count = 125   Avg Score = 0.8321447877396573
My reducer. Feature Count = 130   Avg Score = 0.8395659380179504
My reducer. Feature Count = 135   Avg Score = 0.8395659380179504
My reducer. Feature Count = 140   Avg Score = 0.8487555787865385
My reducer. Feature Count = 145   Avg Score = 0.8570262554782679
My reducer. Feature Count = 150   Avg Score = 0.8570262554782679
My reducer. Feature Count = 155   Avg Score = 0.8652969321699971
My reducer. Feature Count = 160   Avg Score = 0.8652969321699971
My reducer. Feature Count = 165   Avg Score = 0.8652969321699971
My reducer. Feature Count = 170   Avg Score = 0.8652969321699971
My reducer. Feature Count = 175   Avg Score = 0.8652969321699971
My reducer. Feature Count = 180   Avg Score = 0.8652969321699971
My reducer. Feature Count = 185   Avg Score = 0.8652969321699971
My reducer. Feature Count = 190   Avg Score = 0.8652969321699971
My reducer. Feature Count = 195   Avg Score = 0.872779925367276
My reducer. Feature Count = 200   Avg Score = 0.872779925367276
My reducer. Feature Count = 205   Avg Score = 0.872779925367276
My reducer. Feature Count = 210   Avg Score = 0.872779925367276
My reducer. Feature Count = 215   Avg Score = 0.872779925367276
My reducer. Feature Count = 220   Avg Score = 0.8450488329303012
My reducer. Feature Count = 225   Avg Score = 0.8450488329303012
My reducer. Feature Count = 230   Avg Score = 0.8211937406541564
My reducer. Feature Count = 235   Avg Score = 0.8314645156308138
My reducer. Feature Count = 240   Avg Score = 0.8314645156308138
My reducer. Feature Count = 245   Avg Score = 0.8397351923225431
My reducer. Feature Count = 250   Avg Score = 0.833171163202123
My reducer. Feature Count = 255   Avg Score = 0.833171163202123
My reducer. Feature Count = 260   Avg Score = 0.833171163202123
My reducer. Feature Count = 265   Avg Score = 0.8489248330911313
My reducer. Feature Count = 270   Avg Score = 0.8571955097828606
My reducer. Feature Count = 275   Avg Score = 0.8571955097828606
My reducer. Feature Count = 280   Avg Score = 0.8571955097828606
My reducer. Feature Count = 285   Avg Score = 0.8571955097828606
My reducer. Feature Count = 290   Avg Score = 0.8571955097828606
My reducer. Feature Count = 295   Avg Score = 0.8571955097828606
My reducer. Feature Count = 300   Avg Score = 0.8571955097828606
My reducer. Feature Count = 305   Avg Score = 0.8497125165855817
My reducer. Feature Count = 310   Avg Score = 0.8414418398938522
My reducer. Feature Count = 315   Avg Score = 0.8414418398938522
My reducer. Feature Count = 320   Avg Score = 0.8496051051999747
My reducer. Feature Count = 325   Avg Score = 0.8496051051999747
My reducer. Feature Count = 330   Avg Score = 0.8496051051999747
My reducer. Feature Count = 335   Avg Score = 0.8496051051999747
My reducer. Feature Count = 340   Avg Score = 0.8570262554782679
My reducer. Feature Count = 345   Avg Score = 0.8570262554782679
My reducer. Feature Count = 350   Avg Score = 0.8570262554782679
My reducer. Feature Count = 355   Avg Score = 0.8570262554782679
My reducer. Feature Count = 360   Avg Score = 0.8570262554782679
My reducer. Feature Count = 365   Avg Score = 0.8570262554782679
My reducer. Feature Count = 370   Avg Score = 0.8570262554782679
My reducer. Feature Count = 375   Avg Score = 0.8570262554782679
My reducer. Feature Count = 380   Avg Score = 0.8570262554782679
My reducer. Feature Count = 385   Avg Score = 0.8570262554782679
My reducer. Feature Count = 390   Avg Score = 0.8570262554782679
My reducer. Feature Count = 395   Avg Score = 0.8570262554782679
My reducer. Feature Count = 400   Avg Score = 0.8570262554782679
My reducer. Feature Count = 405   Avg Score = 0.8570262554782679
My reducer. Feature Count = 410   Avg Score = 0.8570262554782679
My reducer. Feature Count = 415   Avg Score = 0.8570262554782679
My reducer. Feature Count = 420   Avg Score = 0.8570262554782679
My reducer. Feature Count = 425   Avg Score = 0.8570262554782679
My reducer. Feature Count = 430   Avg Score = 0.8570262554782679
My reducer. Feature Count = 435   Avg Score = 0.8570262554782679
My reducer. Feature Count = 440   Avg Score = 0.8570262554782679
My reducer. Feature Count = 445   Avg Score = 0.8570262554782679
My reducer. Feature Count = 450   Avg Score = 0.8570262554782679
My reducer. Feature Count = 455   Avg Score = 0.8570262554782679
My reducer. Feature Count = 460   Avg Score = 0.8570262554782679
My reducer. Feature Count = 465   Avg Score = 0.8570262554782679
My reducer. Feature Count = 470   Avg Score = 0.8570262554782679
My reducer. Feature Count = 475   Avg Score = 0.8570262554782679
My reducer. Feature Count = 480   Avg Score = 0.8570262554782679
My reducer. Feature Count = 485   Avg Score = 0.8570262554782679
My reducer. Feature Count = 490   Avg Score = 0.8570262554782679
My reducer. Feature Count = 495   Avg Score = 0.8570262554782679
My reducer. Feature Count = 500   Avg Score = 0.8652969321699973
My reducer. Feature Count = 505   Avg Score = 0.8652969321699973
My reducer. Feature Count = 510   Avg Score = 0.8652969321699973
My reducer. Feature Count = 515   Avg Score = 0.8652969321699973
My reducer. Feature Count = 520   Avg Score = 0.8652969321699973
My reducer. Feature Count = 525   Avg Score = 0.8570262554782679
My reducer. Feature Count = 530   Avg Score = 0.8570262554782679
My reducer. Feature Count = 535   Avg Score = 0.8404154644313867
My reducer. Feature Count = 540   Avg Score = 0.8321447877396573
My reducer. Feature Count = 545   Avg Score = 0.8321447877396573
My reducer. Feature Count = 550   Avg Score = 0.8321447877396573
My reducer. Feature Count = 555   Avg Score = 0.8321447877396573
My reducer. Feature Count = 560   Avg Score = 0.8321447877396573
My reducer. Feature Count = 565   Avg Score = 0.8321447877396573
My reducer. Feature Count = 570   Avg Score = 0.8321447877396573
My reducer. Feature Count = 575   Avg Score = 0.8321447877396573
My reducer. Feature Count = 580   Avg Score = 0.8321447877396573
My reducer. Feature Count = 585   Avg Score = 0.8321447877396573
My reducer. Feature Count = 590   Avg Score = 0.8321447877396573
My reducer. Feature Count = 595   Avg Score = 0.8321447877396573
My reducer. Feature Count = 600   Avg Score = 0.8321447877396573
My reducer. Feature Count = 605   Avg Score = 0.8321447877396573
My reducer. Feature Count = 610   Avg Score = 0.8321447877396573
My reducer. Feature Count = 615   Avg Score = 0.8321447877396573
My reducer. Feature Count = 620   Avg Score = 0.8321447877396573
My reducer. Feature Count = 625   Avg Score = 0.8321447877396573
My reducer. Feature Count = 630   Avg Score = 0.8321447877396573
My reducer. Feature Count = 635   Avg Score = 0.8321447877396573
My reducer. Feature Count = 640   Avg Score = 0.8321447877396573
My reducer. Feature Count = 645   Avg Score = 0.8321447877396573
My reducer. Feature Count = 650   Avg Score = 0.8321447877396573
My reducer. Feature Count = 655   Avg Score = 0.8205901658909178
My reducer. Feature Count = 660   Avg Score = 0.8205901658909178
My reducer. Feature Count = 665   Avg Score = 0.8205901658909178
My reducer. Feature Count = 670   Avg Score = 0.8205901658909178
My reducer. Feature Count = 675   Avg Score = 0.8205901658909178
My reducer. Feature Count = 680   Avg Score = 0.8205901658909178
My reducer. Feature Count = 685   Avg Score = 0.8205901658909178
My reducer. Feature Count = 690   Avg Score = 0.8205901658909178
My reducer. Feature Count = 695   Avg Score = 0.8074949277956797
My reducer. Feature Count = 700   Avg Score = 0.8074949277956797
My reducer. Feature Count = 705   Avg Score = 0.8074949277956797
My reducer. Feature Count = 710   Avg Score = 0.8074949277956797
My reducer. Feature Count = 715   Avg Score = 0.8074949277956797
My reducer. Feature Count = 720   Avg Score = 0.8074949277956797
My reducer. Feature Count = 725   Avg Score = 0.8074949277956797
My reducer. Feature Count = 730   Avg Score = 0.8074949277956797
My reducer. Feature Count = 735   Avg Score = 0.8074949277956797
My reducer. Feature Count = 740   Avg Score = 0.8074949277956797
My reducer. Feature Count = 745   Avg Score = 0.8074949277956797
My reducer. Feature Count = 750   Avg Score = 0.8074949277956797
My reducer. Feature Count = 755   Avg Score = 0.7992242511039503
My reducer. Feature Count = 760   Avg Score = 0.7992242511039503
My reducer. Feature Count = 765   Avg Score = 0.7992242511039503
My reducer. Feature Count = 770   Avg Score = 0.7992242511039503
My reducer. Feature Count = 775   Avg Score = 0.7992242511039503
My reducer. Feature Count = 780   Avg Score = 0.7992242511039503
My reducer. Feature Count = 785   Avg Score = 0.7992242511039503
My reducer. Feature Count = 790   Avg Score = 0.7992242511039503
My reducer. Feature Count = 795   Avg Score = 0.7992242511039503
My reducer. Feature Count = 800   Avg Score = 0.7992242511039503
My reducer. Feature Count = 805   Avg Score = 0.7992242511039503
My reducer. Feature Count = 810   Avg Score = 0.7992242511039503
My reducer. Feature Count = 815   Avg Score = 0.7992242511039503
My reducer. Feature Count = 820   Avg Score = 0.7992242511039503
My reducer. Feature Count = 825   Avg Score = 0.7992242511039503
My reducer. Feature Count = 830   Avg Score = 0.7992242511039503
My reducer. Feature Count = 835   Avg Score = 0.7992242511039503
My reducer. Feature Count = 840   Avg Score = 0.8074949277956797
My reducer. Feature Count = 845   Avg Score = 0.8074949277956797
My reducer. Feature Count = 850   Avg Score = 0.8074949277956797
My reducer. Feature Count = 855   Avg Score = 0.8074949277956797
My reducer. Feature Count = 860   Avg Score = 0.8074949277956797
My reducer. Feature Count = 865   Avg Score = 0.7992242511039505
My reducer. Feature Count = 870   Avg Score = 0.7992242511039505
My reducer. Feature Count = 875   Avg Score = 0.7992242511039505
My reducer. Feature Count = 880   Avg Score = 0.791060985797828
My reducer. Feature Count = 885   Avg Score = 0.8041562238930661
My reducer. Feature Count = 890   Avg Score = 0.8041562238930661
My reducer. Feature Count = 895   Avg Score = 0.8041562238930661
My reducer. Feature Count = 900   Avg Score = 0.8041562238930661
My reducer. Feature Count = 905   Avg Score = 0.8041562238930661
My reducer. Feature Count = 910   Avg Score = 0.8041562238930661
My reducer. Feature Count = 915   Avg Score = 0.8041562238930661
My reducer. Feature Count = 920   Avg Score = 0.8041562238930661
My reducer. Feature Count = 925   Avg Score = 0.794966583124478
My reducer. Feature Count = 930   Avg Score = 0.794966583124478
My reducer. Feature Count = 935   Avg Score = 0.8032372598162072
My reducer. Feature Count = 940   Avg Score = 0.8032372598162072
My reducer. Feature Count = 945   Avg Score = 0.8032372598162072
My reducer. Feature Count = 950   Avg Score = 0.8032372598162072
My reducer. Feature Count = 955   Avg Score = 0.8114005251223296
My reducer. Feature Count = 960   Avg Score = 0.8031298484306004
My reducer. Feature Count = 965   Avg Score = 0.8031298484306004
My reducer. Feature Count = 970   Avg Score = 0.8031298484306004
My reducer. Feature Count = 975   Avg Score = 0.8031298484306004
My reducer. Feature Count = 980   Avg Score = 0.8031298484306004
My reducer. Feature Count = 985   Avg Score = 0.8031298484306004
My reducer. Feature Count = 990   Avg Score = 0.8031298484306004
My reducer. Feature Count = 995   Avg Score = 0.8031298484306004
My reducer. Feature Count = 1000   Avg Score = 0.8031298484306004

====================================================================================
= My custom NB model, J = # of features, using chi-squared, alpha = 0.0001 and random
= oversampling to create a balanced data set. 5-fold cross validation.
====================================================================================
Finished loading and sampling. Data dist = Counter({0.0: 722, 1.0: 722})
J = 5,     F1 =     0.7854361533214795
J = 10,     F1 =     0.7479625393802618
J = 15,     F1 =     0.7661814753018514
J = 20,     F1 =     0.7611895845027334
J = 25,     F1 =     0.786468872938244
J = 30,     F1 =     0.7778331781110064
J = 35,     F1 =     0.7723860164604186
J = 40,     F1 =     0.7622435763626465
J = 45,     F1 =     0.7642373200368778
J = 50,     F1 =     0.7690207123939853
J = 55,     F1 =     0.7620510981652318
J = 60,     F1 =     0.7776638339764952
J = 65,     F1 =     0.777004501422318
J = 70,     F1 =     0.7861880837511533
J = 75,     F1 =     0.7763284409525981
J = 80,     F1 =     0.7791272363314276
J = 85,     F1 =     0.7893353055900523
J = 90,     F1 =     0.7872120889531378
J = 95,     F1 =     0.7950314349074045
J = 100,     F1 =     0.7910302290792163



====================================================================================
= My custom NB model, J = # of features, using chi-squared, alpha = 0.0001 and random
= oversampling to create a balanced data set. 5-fold cross validation (Sane as above)
= These are probably invalid due to my oversampling method.
====================================================================================
J = 1,     F1 =     0.7089615406867829
J = 2,     F1 =     0.7431265008269751
J = 3,     F1 =     0.7618512800657866
J = 4,     F1 =     0.7778057389576769
J = 5,     F1 =     0.7842440334080253
J = 6,     F1 =     0.7780769128963273
J = 7,     F1 =     0.7454441869971241
J = 8,     F1 =     0.7439197980422857
J = 9,     F1 =     0.7474306071958836
J = 10,     F1 =     0.7466177560777377
J = 11,     F1 =     0.7542558384154093
J = 12,     F1 =     0.7559379945713499
J = 13,     F1 =     0.7484900891709002
J = 14,     F1 =     0.7594093994122482
J = 15,     F1 =     0.7677107556289016
J = 16,     F1 =     0.7633033112387368
J = 17,     F1 =     0.7628309523473449
J = 18,     F1 =     0.7579702039576812
J = 19,     F1 =     0.7579113442271337
J = 20,     F1 =     0.7603125805003969
J = 22,     F1 =     0.7862630116863814
J = 24,     F1 =     0.786769742462931
J = 26,     F1 =     0.7861856687983944
J = 28,     F1 =     0.7836049247744377
J = 30,     F1 =     0.778684303512452
J = 32,     F1 =     0.7679467081005186
J = 34,     F1 =     0.7657220383118226
J = 36,     F1 =     0.7654508535873475
J = 38,     F1 =     0.7655008033268345
J = 40,     F1 =     0.765687796616561
J = 42,     F1 =     0.7627425326384725
J = 44,     F1 =     0.7634467048013812
J = 46,     F1 =     0.7675230227710662
J = 48,     F1 =     0.7659752022473054
J = 50,     F1 =     0.7643209568649018
J = 55,     F1 =     0.7638064261243179
J = 60,     F1 =     0.781256790385114
J = 65,     F1 =     0.778289163299597
J = 70,     F1 =     0.7890783091312403
J = 75,     F1 =     0.7779259933023421
J = 80,     F1 =     0.7801745763590293
J = 85,     F1 =     0.7844689761534445
J = 90,     F1 =     0.787148166983937
J = 95,     F1 =     0.7894064179618872
J = 100,     F1 =     0.7896317390527975
J = 105,     F1 =     0.8052881861278424
J = 110,     F1 =     0.814216996804003
J = 115,     F1 =     0.801299302455913
J = 120,     F1 =     0.8092046761119489
J = 125,     F1 =     0.8144191433152506
J = 130,     F1 =     0.8169551350361004
J = 135,     F1 =     0.8185999584771493
J = 140,     F1 =     0.8240725494727738
J = 145,     F1 =     0.8202551714114353
J = 150,     F1 =     0.8240887113170924
J = 160,     F1 =     0.8256891522372503
J = 170,     F1 =     0.828437064552871
J = 180,     F1 =     0.8294229948805232
J = 190,     F1 =     0.8445477277254481
J = 200,     F1 =     0.8416684799095856
J = 210,     F1 =     0.840298591598262
J = 220,     F1 =     0.8435915391823837
J = 230,     F1 =     0.8521109431136409
J = 240,     F1 =     0.8592507972230493
J = 250,     F1 =     0.8589120859918615
J = 260,     F1 =     0.8541489477829212
J = 270,     F1 =     0.8577556753587192
J = 280,     F1 =     0.8620003471512423
J = 290,     F1 =     0.8617095378903595
J = 300,     F1 =     0.874322527794844
J = 310,     F1 =     0.8761347413427825
J = 320,     F1 =     0.8763534061717351
J = 330,     F1 =     0.8743435714177668
J = 340,     F1 =     0.8819652251714644
J = 350,     F1 =     0.8881725567547827
J = 360,     F1 =     0.9002285456090695
J = 370,     F1 =     0.9035737932810889
J = 380,     F1 =     0.9039768370686694
J = 390,     F1 =     0.9003307410883599
J = 400,     F1 =     0.9033232725622113
J = 410,     F1 =     0.902787229395263
J = 420,     F1 =     0.9041807829399428
J = 430,     F1 =     0.9029917896219535
J = 440,     F1 =     0.9118851676790604
J = 450,     F1 =     0.9092619064576091
J = 460,     F1 =     0.9079746592591722
J = 470,     F1 =     0.9142531535312093
J = 480,     F1 =     0.9156892974961727
J = 490,     F1 =     0.9145535066283437
J = 500,     F1 =     0.9177060507848791
J = 525,     F1 =     0.9164635391666606
J = 550,     F1 =     0.9166314452143629
J = 575,     F1 =     0.930235493805667
J = 600,     F1 =     0.9292542487033906
J = 625,     F1 =     0.9249004074710075
J = 650,     F1 =     0.9287927285127985
J = 675,     F1 =     0.9292871719269595
J = 700,     F1 =     0.9281454551784485
J = 725,     F1 =     0.9326308214832888
J = 750,     F1 =     0.9312918136847932
J = 775,     F1 =     0.9360447566071451
J = 800,     F1 =     0.9331215262976592
J = 825,     F1 =     0.9361735987434923
J = 850,     F1 =     0.9332172473513015
J = 875,     F1 =     0.9327266686323593
J = 900,     F1 =     0.9323413778948277
J = 925,     F1 =     0.9337698852118198
J = 950,     F1 =     0.937599419550639
J = 975,     F1 =     0.9328665549585704
J = 1000,     F1 =     0.9333727954957762
J = 1050,     F1 =     0.9350832951624287
J = 1100,     F1 =     0.9409782050362818
J = 1150,     F1 =     0.9366766049357438
J = 1200,     F1 =     0.9348942715313628
J = 1250,     F1 =     0.9378922437926882
J = 1300,     F1 =     0.9426402622505027
J = 1350,     F1 =     0.9399859318390682
J = 1400,     F1 =     0.9368434494767903
J = 1450,     F1 =     0.9368165866203189
J = 1500,     F1 =     0.9375695677077683
J = 1550,     F1 =     0.9466758940544496
J = 1600,     F1 =     0.9472555094289975
J = 1650,     F1 =     0.9419766033831787
J = 1700,     F1 =     0.949197985694678
J = 1750,     F1 =     0.9477229798713294
J = 1800,     F1 =     0.9519826113632917
J = 1850,     F1 =     0.9525914584407129
J = 1900,     F1 =     0.9531405872854546
J = 1950,     F1 =     0.9513368306568951
J = 2000,     F1 =     0.9517552753448953
J = 2100,     F1 =     0.9540792920093054
J = 2200,     F1 =     0.9531340795144168
J = 2300,     F1 =     0.9551256480871644
J = 2400,     F1 =     0.9541500424332702
J = 2500,     F1 =     0.9573742780381659
J = 2600,     F1 =     0.953746329666776
J = 2700,     F1 =     0.9522612509257069
J = 2800,     F1 =     0.9616861018183288
J = 2900,     F1 =     0.9625011997218804
J = 3000,     F1 =     0.9620479359173297
J = 3100,     F1 =     0.964221166145254
J = 3200,     F1 =     0.9657045302578044
J = 3300,     F1 =     0.9662948407005828
J = 3400,     F1 =     0.9656086529698218
J = 3500,     F1 =     0.965318536631124
J = 3600,     F1 =     0.9668922135752338
J = 3700,     F1 =     0.9662910981249869
J = 3800,     F1 =     0.9664241994701837
J = 3900,     F1 =     0.9646638581507684
J = 4000,     F1 =     0.9651995148806263
J = 4100,     F1 =     0.970330546546141
J = 4200,     F1 =     0.9730715074976883
J = 4300,     F1 =     0.9729400657561815
J = 4400,     F1 =     0.9704364052049954
J = 4500,     F1 =     0.9691910466278502
J = 4600,     F1 =     0.9699818746350793
J = 4700,     F1 =     0.9660176201963491
J = 4800,     F1 =     0.9694564846671584
J = 4900,     F1 =     0.9654910150690693
J = 5000,     F1 =     0.9655242987188473
J = 5250,     F1 =     0.968746986379682
J = 5500,     F1 =     0.9697927796103872
J = 5750,     F1 =     0.9709108750594005
J = 6000,     F1 =     0.9707569074455608
J = 6250,     F1 =     0.9697998422708686
J = 6500,     F1 =     0.971204665799583
J = 6750,     F1 =     0.9721355759608153
J = 7000,     F1 =     0.971062420671592
J = 7250,     F1 =     0.9717153494829857
J = 7500,     F1 =     0.9746036006835903
J = 7750,     F1 =     0.9738772611220055
J = 8000,     F1 =     0.9729974905408542
J = 8250,     F1 =     0.9732896145261017
J = 8500,     F1 =     0.9752185662707034
J = 8750,     F1 =     0.9762598110974465
J = 9000,     F1 =     0.9779254026468174
J = 9250,     F1 =     0.9785729507625162
J = 9500,     F1 =     0.9790063344248076
J = 9750,     F1 =     0.9824461829158769
J = 10000,     F1 =     0.9858899723459329
J = 10000,     F1 =     0.9825003408084461
J = 10500,     F1 =     0.9831417904738018
J = 11000,     F1 =     0.9823786015104343
J = 11500,     F1 =     0.9848357998173615
J = 12000,     F1 =     0.9822104615398846
J = 12500,     F1 =     0.9846829060788403
J = 13000,     F1 =     0.9812843192870442
J = 13500,     F1 =     0.9810351589571127
J = 14000,     F1 =     0.9817743172206409
J = 14500,     F1 =     0.9801861445163944
J = 15000,     F1 =     0.9823635269331559

###########################################
# Random undersampling with Naive Bayes
###########################################
Counter(y) = Counter({0.0: 722, 1.0: 78})
Counter(y_res) = Counter({0.0: 78, 1.0: 78})
J = 1,     F1 =     0.7151282051282052
J = 2,     F1 =     0.7481481481481482
J = 3,     F1 =     0.775156695156695
J = 4,     F1 =     0.775156695156695
J = 5,     F1 =     0.7833287381674477
J = 6,     F1 =     0.7603087951475048
J = 7,     F1 =     0.740616487455197
J = 8,     F1 =     0.7539498207885303
J = 9,     F1 =     0.7618863287250383
J = 10,     F1 =     0.7820344768731864
J = 11,     F1 =     0.7820344768731864
J = 12,     F1 =     0.7910291858678955
J = 13,     F1 =     0.7910291858678955
J = 14,     F1 =     0.7861904761904761
J = 15,     F1 =     0.7935796387520525
J = 16,     F1 =     0.7861904761904761
J = 17,     F1 =     0.7937662337662338
J = 18,     F1 =     0.7937662337662338
J = 19,     F1 =     0.7937662337662338
J = 20,     F1 =     0.8204067332262058
J = 22,     F1 =     0.8314739269020951
J = 24,     F1 =     0.8407046961328642
J = 26,     F1 =     0.8240847643405187
J = 28,     F1 =     0.8240847643405187
J = 30,     F1 =     0.8292680445491539
J = 32,     F1 =     0.8292680445491539
J = 34,     F1 =     0.8218788819875776
J = 36,     F1 =     0.8218788819875776
J = 38,     F1 =     0.8218788819875776
J = 40,     F1 =     0.8218788819875776
J = 42,     F1 =     0.8221149422236378
J = 44,     F1 =     0.8221149422236378
J = 46,     F1 =     0.8221149422236378
J = 48,     F1 =     0.815593203093203
J = 50,     F1 =     0.815593203093203
J = 55,     F1 =     0.815593203093203
J = 60,     F1 =     0.815593203093203
J = 65,     F1 =     0.8110477485477485
J = 70,     F1 =     0.8200424575424575
J = 75,     F1 =     0.8200424575424575
J = 80,     F1 =     0.8272394272394272
J = 85,     F1 =     0.8357864357864357
J = 90,     F1 =     0.846853629462325
J = 95,     F1 =     0.846853629462325
J = 100,     F1 =     0.846853629462325
J = 105,     F1 =     0.846853629462325
J = 110,     F1 =     0.846853629462325
J = 115,     F1 =     0.8795357223349111
J = 120,     F1 =     0.8795357223349111
J = 125,     F1 =     0.8795357223349111
J = 130,     F1 =     0.8888690556682443
J = 135,     F1 =     0.8888690556682443
J = 140,     F1 =     0.8888690556682443
J = 145,     F1 =     0.8888690556682443
J = 150,     F1 =     0.8888690556682443
J = 160,     F1 =     0.8957656073923822
J = 170,     F1 =     0.8957656073923822
J = 180,     F1 =     0.9037021153288901
J = 190,     F1 =     0.9037021153288901
J = 200,     F1 =     0.9037021153288901
J = 210,     F1 =     0.910088669950739
J = 220,     F1 =     0.9037021153288901
J = 230,     F1 =     0.8968055636047524
J = 240,     F1 =     0.9037021153288901
J = 250,     F1 =     0.9037021153288901
J = 260,     F1 =     0.9037021153288901
J = 270,     F1 =     0.8958029556650248
J = 280,     F1 =     0.910088669950739
J = 290,     F1 =     0.9036190476190477
J = 300,     F1 =     0.8958029556650248
J = 310,     F1 =     0.8958029556650248
J = 320,     F1 =     0.8958029556650248
J = 330,     F1 =     0.9183200381376132
J = 340,     F1 =     0.9183200381376132
J = 350,     F1 =     0.9183200381376132
J = 360,     F1 =     0.9183200381376132
J = 370,     F1 =     0.9252165898617513
J = 380,     F1 =     0.9338319744771357
J = 390,     F1 =     0.9338319744771357
J = 400,     F1 =     0.9338319744771357
J = 410,     F1 =     0.9338319744771357
J = 420,     F1 =     0.9338319744771357
J = 430,     F1 =     0.9338319744771357
J = 440,     F1 =     0.9406868131868131
J = 450,     F1 =     0.9531579531579532
J = 460,     F1 =     0.9531579531579532
J = 470,     F1 =     0.9531579531579532
J = 480,     F1 =     0.9531579531579532
J = 490,     F1 =     0.9531579531579532
J = 500,     F1 =     0.9531579531579532
J = 525,     F1 =     0.9611351611351612
J = 550,     F1 =     0.9611351611351612
J = 575,     F1 =     0.9551034151034152
J = 600,     F1 =     0.9611351611351612
J = 625,     F1 =     0.975931731104145
J = 650,     F1 =     0.975931731104145
J = 675,     F1 =     0.975931731104145
J = 700,     F1 =     0.981992337164751
J = 725,     F1 =     0.981992337164751
J = 750,     F1 =     0.981992337164751
J = 775,     F1 =     0.981992337164751
J = 800,     F1 =     0.981992337164751
J = 825,     F1 =     0.981992337164751
J = 850,     F1 =     0.981992337164751
J = 875,     F1 =     0.9888888888888889
J = 900,     F1 =     0.9888888888888889
J = 925,     F1 =     0.9888888888888889
J = 950,     F1 =     0.9888888888888889
J = 975,     F1 =     0.9888888888888889
J = 1000,     F1 =     0.9888888888888889
J = 1050,     F1 =     0.9888888888888889
J = 1100,     F1 =     0.9888888888888889
J = 1150,     F1 =     0.9888888888888889
J = 1200,     F1 =     0.9888888888888889
J = 1250,     F1 =     0.9942857142857143
J = 1300,     F1 =     0.9942857142857143
J = 1350,     F1 =     0.9942857142857143
J = 1400,     F1 =     0.9942857142857143
J = 1450,     F1 =     0.9942857142857143
J = 1500,     F1 =     0.9942857142857143
J = 1550,     F1 =     0.9942857142857143
J = 1600,     F1 =     0.9942857142857143
J = 1650,     F1 =     0.9942857142857143
J = 1700,     F1 =     0.9942857142857143
J = 1750,     F1 =     0.9942857142857143
J = 1800,     F1 =     0.9942857142857143
J = 1850,     F1 =     0.9942857142857143
J = 1900,     F1 =     0.9942857142857143
J = 1950,     F1 =     0.9942857142857143
J = 2000,     F1 =     0.9942857142857143
J = 2100,     F1 =     0.9942857142857143
J = 2200,     F1 =     0.9942857142857143
J = 2300,     F1 =     0.9942857142857143
J = 2400,     F1 =     0.9942857142857143
J = 2500,     F1 =     0.9942857142857143
J = 2600,     F1 =     0.9942857142857143
J = 2700,     F1 =     0.9942857142857143
J = 2800,     F1 =     0.9942857142857143
J = 2900,     F1 =     0.9942857142857143
J = 3000,     F1 =     0.9942857142857143
J = 3100,     F1 =     0.9942857142857143
J = 3200,     F1 =     0.9942857142857143
J = 3300,     F1 =     0.9942857142857143
J = 3400,     F1 =     0.9942857142857143
J = 3500,     F1 =     0.9942857142857143
J = 3600,     F1 =     0.9942857142857143
J = 3700,     F1 =     0.9942857142857143
J = 3800,     F1 =     0.9942857142857143

#######################################################
# Multi-model, with 9 models, chi2. 
#######################################################
F1 Score for FR size = 55 is: 0.6808873044167162
F1 Score for FR size = 105 is: 0.7016068610650826
F1 Score for FR size = 155 is: 0.682061851864901
F1 Score for FR size = 205 is: 0.6767105446901712
F1 Score for FR size = 255 is: 0.67003663003663
F1 Score for FR size = 305 is: 0.6622689075630253
F1 Score for FR size = 375 is: 0.6622689075630253
F1 Score for FR size = 505 is: 0.6604761904761904
F1 Score for FR size = 655 is: 0.6507046070460705
F1 Score for FR size = 805 is: 0.6699087148766224
F1 Score for FR size = 1005 is: 0.6399771968543574
F1 Score for FR size = 1305 is: 0.6130954244645271




############################################
# Decision Tree Classifier, Feature Count = 1331, Truncated SVD. These are the k-fold results.
############################################
f1 = 0.3776647605228077
f1 = 0.4163142265749868
f1 = 0.36893242125276376
f1 = 0.39697332067599167
f1 = 0.34638515793695224



############################################
# 331 features, Chi Squared
############################################

Max Depth = 2
f1 Score list = [0.7333333333333334, 0.48, 0.5999999999999999, 0.6666666666666666, 0.7142857142857143]
Mean = 0.6388571428571428


Max Depth = 3
f1 Score list = [0.6896551724137931, 0.46153846153846156, 0.5625, 0.6896551724137931, 0.5517241379310345]
Mean = 0.5910145888594165


Max Depth = 4
f1 Score list = [0.6666666666666667, 0.4827586206896552, 0.5625, 0.6428571428571429, 0.5185185185185185]
Mean = 0.5746601897463967

Max Depth = 5
f1 Score list = [0.6000000000000001, 0.4827586206896552, 0.5454545454545455, 0.6428571428571429, 0.4799999999999999]
Mean = 0.5502140618002688



############################################
# Decision Trees - Max Depth, chi2 - full data
############################################
Mean for feature and depth of (1, 2) = 0.6192673992673993
Mean for feature and depth of (3, 2) = 0.6353571428571428
Mean for feature and depth of (7, 2) = 0.6404146141215107
Mean for feature and depth of (15, 2) = 0.6254150419473
Mean for feature and depth of (31, 2) = 0.6330710955710955
Mean for feature and depth of (63, 2) = 0.6241666666666666
Mean for feature and depth of (127, 2) = 0.6333333333333333
Mean for feature and depth of (255, 2) = 0.6388571428571428
Mean for feature and depth of (511, 2) = 0.6285714285714286
Mean for feature and depth of (1023, 2) = 0.6142857142857143
Mean for feature and depth of (2047, 2) = 0.6285714285714286

Depth = 3
Mean for feature and depth of (1, 3) = 0.6192673992673993
Mean for feature and depth of (3, 3) = 0.65213133640553
Mean for feature and depth of (7, 3) = 0.6328578947933787
Mean for feature and depth of (15, 3) = 0.6573650483327903
Mean for feature and depth of (31, 3) = 0.6050913713650088
Mean for feature and depth of (63, 3) = 0.571199863457928
Mean for feature and depth of (127, 3) = 0.6035714285714285
Mean for feature and depth of (255, 3) = 0.6047321097515759
Mean for feature and depth of (511, 3) = 0.5781443667567138
Mean for feature and depth of (1023, 3) = 0.5837950897822978
Mean for feature and depth of (2047, 3) = 0.6078417096437119

Depth of 4
Mean for feature and depth of (1, 4) = 0.6192673992673993
Mean for feature and depth of (3, 4) = 0.65213133640553
Mean for feature and depth of (7, 4) = 0.6132554422877003
Mean for feature and depth of (15, 4) = 0.6002105946815754
Mean for feature and depth of (31, 4) = 0.5984543672864029
Mean for feature and depth of (63, 4) = 0.6023399014778326
Mean for feature and depth of (127, 4) = 0.5608294930875576
Mean for feature and depth of (255, 4) = 0.5632302069454461
Mean for feature and depth of (511, 4) = 0.5270830319106181
Mean for feature and depth of (1023, 4) = 0.5681416313674379
Mean for feature and depth of (2047, 4) = 0.6058927031223046

Depth of 5
Mean for feature and depth of (1, 5) = 0.6192673992673993
Mean for feature and depth of (3, 5) = 0.65213133640553
Mean for feature and depth of (7, 5) = 0.6216332686920922
Mean for feature and depth of (15, 5) = 0.5665550124904963
Mean for feature and depth of (31, 5) = 0.5579944070266651
Mean for feature and depth of (63, 5) = 0.5638787878787879
Mean for feature and depth of (127, 5) = 0.5517972350230416
Mean for feature and depth of (255, 5) = 0.5391029975208474
Mean for feature and depth of (511, 5) = 0.6021273490239007
Mean for feature and depth of (1023, 5) = 0.5478051450465243
Mean for feature and depth of (2047, 5) = 0.5459484636766584

######################################
# Truncated SVD Decision Tree
######################################
Mean for feature and depth of (1, 4) = 0.20736806264896152
Mean for feature and depth of (3, 4) = 0.45932773109243696
Mean for feature and depth of (7, 4) = 0.5781209150326798
Mean for feature and depth of (15, 4) = 0.5434719527121832
Mean for feature and depth of (31, 4) = 0.5416723531357678
Mean for feature and depth of (63, 4) = 0.5384740922072623
Mean for feature and depth of (127, 4) = 0.5891326844815217
Mean for feature and depth of (255, 4) = 0.5486865167352972
Mean for feature and depth of (511, 4) = 0.5711688311688312
Mean for feature and depth of (1023, 4) = 0.5831681946460642
Mean for feature and depth of (2047, 4) = 0.5422720736114815

#######################################
# Naive Bayes with Information Gain, multi-model with 13 models
#######################################
F1 Score for FR size = 55 is: 0.6771467207572684
F1 Score for FR size = 105 is: 0.7019734345351044
F1 Score for FR size = 155 is: 0.6884670231729055
F1 Score for FR size = 205 is: 0.7113192594110815
F1 Score for FR size = 255 is: 0.7225899696487932
F1 Score for FR size = 305 is: 0.7187910953855227
F1 Score for FR size = 375 is: 0.7257175023119296
F1 Score for FR size = 505 is: 0.731364230844395
F1 Score for FR size = 655 is: 0.7460010491344641
F1 Score for FR size = 805 is: 0.7658547519762824
F1 Score for FR size = 1005 is: 0.7649095857924381
F1 Score for FR size = 1305 is: 0.7540132382237645

Same.
F1 Score for FR size = 1 is: 0.639047619047619
F1 Score for FR size = 2 is: 0.6613756613756614
F1 Score for FR size = 3 is: 0.6706498549240485
F1 Score for FR size = 5 is: 0.68128078817734
F1 Score for FR size = 7 is: 0.6423357035121742
F1 Score for FR size = 11 is: 0.643271105206589
F1 Score for FR size = 15 is: 0.6850584898971995
F1 Score for FR size = 31 is: 0.6865043002974037
F1 Score for FR size = 47 is: 0.6701291768976193
F1 Score for FR size = 63 is: 0.6758717621453996
F1 Score for FR size = 95 is: 0.7019734345351044
F1 Score for FR size = 127 is: 0.6920320855614973
F1 Score for FR size = 191 is: 0.6985461501673841
F1 Score for FR size = 255 is: 0.7225899696487932
F1 Score for FR size = 383 is: 0.7222928811164105
F1 Score for FR size = 511 is: 0.731364230844395
F1 Score for FR size = 767 is: 0.7572526014386479
F1 Score for FR size = 1023 is: 0.7649095857924381
F1 Score for FR size = 1535 is: 0.7302366381068951
F1 Score for FR size = 2047 is: 0.726639463192798
F1 Score for FR size = 3071 is: 0.7384095474418055
F1 Score for FR size = 4095 is: 0.7456484465470344

############################################################
# Tuning the model Count, NB, Feature reduction size = 805
############################################################
F1 Score for FR size = 805 and model count = 7 is: 0.7658547519762824
F1 Score for FR size = 805 and model count = 9 is: 0.7658547519762824
F1 Score for FR size = 805 and model count = 13 is: 0.7658547519762824
F1 Score for FR size = 805 and model count = 19 is: 0.7658547519762824
F1 Score for FR size = 805 and model count = 27 is: 0.7658547519762824
F1 Score for FR size = 805 and model count = 37 is: 0.7615873266562255

############################################################
# Tuning NN with Chi2 reduction, 1 hidden layer = sqrt(featureCount), alpha = 0.00001
############################################################
Cross validation score for MLP Classifier with feature size = 11 is: 0.5751092707614447
Cross validation score for MLP Classifier with feature size = 15 is: 0.6062316094699903
Cross validation score for MLP Classifier with feature size = 31 is: 0.5948358327039953
Cross validation score for MLP Classifier with feature size = 47 is: 0.6110855683269476
Cross validation score for MLP Classifier with feature size = 63 is: 0.5068982259570496
Cross validation score for MLP Classifier with feature size = 95 is: 0.5531135531135531
Cross validation score for MLP Classifier with feature size = 127 is: 0.5761222051544632
Cross validation score for MLP Classifier with feature size = 191 is: 0.6001338644377693
Cross validation score for MLP Classifier with feature size = 255 is: 0.589280885498203
Cross validation score for MLP Classifier with feature size = 383 is: 0.5040010786395659
Cross validation score for MLP Classifier with feature size = 511 is: 0.5092595459236327
Cross validation score for MLP Classifier with feature size = 767 is: 0.5487976539589442
Cross validation score for MLP Classifier with feature size = 1023 is: 0.6016427718040621
Cross validation score for MLP Classifier with feature size = 1535 is: 0.6807881773399014
Cross validation score for MLP Classifier with feature size = 2047 is: 0.7066874304783093
Cross validation score for MLP Classifier with feature size = 3071 is: 0.7103655913978495
Cross validation score for MLP Classifier with feature size = 4095 is: 0.7084232804232804
Cross validation score for MLP Classifier with feature size = 6143 is: 0.7162991031956549
Cross validation score for MLP Classifier with feature size = 8191 is: 0.7124606828099598

##############################################################
# Tuning NN with Custom Information Gain reduction, 1 hidden layer = sqrt(featureCount), alpha = 0.00001
##############################################################
Cross validation score for MLP Classifier with feature size = 11 is: 0.5938901516983444
Cross validation score for MLP Classifier with feature size = 15 is: 0.6429032321446114
Cross validation score for MLP Classifier with feature size = 31 is: 0.5918704950963016
Cross validation score for MLP Classifier with feature size = 47 is: 0.6035411255411256
Cross validation score for MLP Classifier with feature size = 63 is: 0.6535913978494623
Cross validation score for MLP Classifier with feature size = 95 is: 0.5380447410703251
Cross validation score for MLP Classifier with feature size = 127 is: 0.5667766310768338
Cross validation score for MLP Classifier with feature size = 191 is: 0.583255524431995
Cross validation score for MLP Classifier with feature size = 255 is: 0.5471485164524214
Cross validation score for MLP Classifier with feature size = 383 is: 0.5449394728651694
Cross validation score for MLP Classifier with feature size = 511 is: 0.49252164502164497
Cross validation score for MLP Classifier with feature size = 767 is: 0.5546543778801845
Cross validation score for MLP Classifier with feature size = 1023 is: 0.5497838601064406
Cross validation score for MLP Classifier with feature size = 1535 is: 0.7592282741795273
Cross validation score for MLP Classifier with feature size = 2047 is: 0.7947478991596639
Cross validation score for MLP Classifier with feature size = 3071 is: 0.7713712810487003
Cross validation score for MLP Classifier with feature size = 4095 is: 0.7592315270935961
Cross validation score for MLP Classifier with feature size = 6143 is: 0.8126310980454473
Cross validation score for MLP Classifier with feature size = 8191 is: 0.7836526181353769


##############################################################
# Tuning NN with TruncatedSVD, 1 hidden layer = sqrt(featureCount), alpha = 0.00001
##############################################################
Cross validation score for MLP Classifier with feature size = 11 is: 0.5178889943074004
Cross validation score for MLP Classifier with feature size = 15 is: 0.685598139346749
Cross validation score for MLP Classifier with feature size = 31 is: 0.5005116562754134
Cross validation score for MLP Classifier with feature size = 47 is: 0.5650525525525525
Cross validation score for MLP Classifier with feature size = 63 is: 0.545664391080867
Cross validation score for MLP Classifier with feature size = 95 is: 0.5421602948737674
Cross validation score for MLP Classifier with feature size = 127 is: 0.6169226311214141
Cross validation score for MLP Classifier with feature size = 191 is: 0.5537761820297971
Cross validation score for MLP Classifier with feature size = 255 is: 0.5786279461279461
Cross validation score for MLP Classifier with feature size = 383 is: 0.6016453201970443
Cross validation score for MLP Classifier with feature size = 511 is: 0.5353905611296915
Cross validation score for MLP Classifier with feature size = 767 is: 0.3497379813169287
Cross validation score for MLP Classifier with feature size = 1023 is: 0.28565980167810834
...
Algorithm is very slow and results are bad, so I cut it off.

#############################################################
# Alpha Tuning - using IGR from above
#############################################################
Cross validation score for MLP Classifier with alpha = 1e-06 and feature size = 6143 is: 0.7801678525816456
Cross validation score for MLP Classifier with alpha = 5e-06 and feature size = 6143 is: 0.812803776683087
Cross validation score for MLP Classifier with alpha = 1e-05 and feature size = 6143 is: 0.8146141215106733
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 6143 is: 0.8286349912601303
Cross validation score for MLP Classifier with alpha = 0.0001 and feature size = 6143 is: 0.7856960408684547
Cross validation score for MLP Classifier with alpha = 0.0005 and feature size = 6143 is: 0.7915270935960591
Cross validation score for MLP Classifier with alpha = 0.001 and feature size = 6143 is: 0.8252246603970741
Cross validation score for MLP Classifier with alpha = 0.005 and feature size = 6143 is: 0.800328407224959
Cross validation score for MLP Classifier with alpha = 0.01 and feature size = 6143 is: 0.7928480204342273
Cross validation score for MLP Classifier with alpha = 0.05 and feature size = 6143 is: 0.7907754059478197
Cross validation score for MLP Classifier with alpha = 0.1 and feature size = 6143 is: 0.8183944535668675

##############################################################
# Neural Network with custom oversampling by feature. I'm skeptical.
##############################################################
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 11 is: 0.958228232624039
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 15 is: 0.9521897934646016
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 31 is: 0.9403754704423115
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 47 is: 0.9493730323217886
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 63 is: 0.9455977158106507
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 95 is: 0.9419122765082995
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 127 is: 0.9407988970547632
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 191 is: 0.9405099357600033
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 255 is: 0.9458709211916643
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 383 is: 0.9468532306518554
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 511 is: 0.9574529108020423
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 767 is: 0.9791894381500811
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 1023 is: 0.9820868924344669
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 1535 is: 0.9828306629142711
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 2047 is: 0.985556005569404
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 3071 is: 0.9896710567258795
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 4095 is: 0.9917377671059755
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 6143 is: 0.9924204576929838
Cross validation score for MLP Classifier with alpha = 5e-05 and feature size = 8191 is: 0.9924347152918921

##############################################################
# Neural Network with Chi2 feature reduction and multi-model with 13 models. VERY SLOW!! About 15 minutes.
##############################################################
F1 Score for NN with Chi2 and FR size = 11 is: 0.6456914440346324
F1 Score for NN with Chi2 and FR size = 15 is: 0.6542857142857142
F1 Score for NN with Chi2 and FR size = 31 is: 0.6278643578643579
F1 Score for NN with Chi2 and FR size = 47 is: 0.5989156789156789
F1 Score for NN with Chi2 and FR size = 63 is: 0.5741659562375675
F1 Score for NN with Chi2 and FR size = 95 is: 0.5771949103528051
F1 Score for NN with Chi2 and FR size = 127 is: 0.5994579945799458
F1 Score for NN with Chi2 and FR size = 191 is: 0.5802983752983752
F1 Score for NN with Chi2 and FR size = 255 is: 0.4947417840375587
F1 Score for NN with Chi2 and FR size = 383 is: 0.5857485481485533
F1 Score for NN with Chi2 and FR size = 511 is: 0.6653495992599061
F1 Score for NN with Chi2 and FR size = 767 is: 0.6635213265622393
F1 Score for NN with Chi2 and FR size = 1023 is: 0.6798911591549993
F1 Score for NN with Chi2 and FR size = 1535 is: 0.6883711926820265
F1 Score for NN with Chi2 and FR size = 2047 is: 0.6505605247465712
F1 Score for NN with Chi2 and FR size = 3071 is: 0.6904761904761905
F1 Score for NN with Chi2 and FR size = 4095 is: 0.6501643239963596
F1 Score for NN with Chi2 and FR size = 6143 is: 0.6779055701469495
F1 Score for NN with Chi2 and FR size = 8191 is: 0.6625347530316474

##############################################################
# Naive Bayes with IGR feature reduction and multi-model with 11 models and undersampling.
#    tuneMultimodelIGR(sizes)
##############################################################
F1 Score for NN with IGR and FR size = 11 is: 0.6575935946651947
F1 Score for NN with IGR and FR size = 15 is: 0.6688833746898263
F1 Score for NN with IGR and FR size = 31 is: 0.6793514328808447
F1 Score for NN with IGR and FR size = 47 is: 0.6646017935304793
F1 Score for NN with IGR and FR size = 63 is: 0.6786340515949955
F1 Score for NN with IGR and FR size = 95 is: 0.6675750483458394
F1 Score for NN with IGR and FR size = 127 is: 0.6507798909037299
F1 Score for NN with IGR and FR size = 191 is: 0.6595085306850013
F1 Score for NN with IGR and FR size = 255 is: 0.6619327731092437
F1 Score for NN with IGR and FR size = 383 is: 0.6482904190221263
F1 Score for NN with IGR and FR size = 511 is: 0.6687245607977315
F1 Score for NN with IGR and FR size = 767 is: 0.6363883500725606
F1 Score for NN with IGR and FR size = 1023 is: 0.6469714335902335
F1 Score for NN with IGR and FR size = 1535 is: 0.6427927927927928
F1 Score for NN with IGR and FR size = 2047 is: 0.6325035173859361
F1 Score for NN with IGR and FR size = 3071 is: 0.6118044818273651
F1 Score for NN with IGR and FR size = 4095 is: 0.5994748270464071

##############################################################
# Naive Bayes with Chi2 feature reduction and multi-model with 11 models and undersampling.
# tuneMultimodelChi2(sizes)
##############################################################
F1 Score for NB with Chi2 and FR size = 11 is: 0.6623247774608937
F1 Score for NB with Chi2 and FR size = 15 is: 0.6688833746898263
F1 Score for NB with Chi2 and FR size = 31 is: 0.6793514328808447
F1 Score for NB with Chi2 and FR size = 47 is: 0.6655382563090474
F1 Score for NB with Chi2 and FR size = 63 is: 0.6803987574773485
F1 Score for NB with Chi2 and FR size = 95 is: 0.6666666666666666
F1 Score for NB with Chi2 and FR size = 127 is: 0.6483474340500207
F1 Score for NB with Chi2 and FR size = 191 is: 0.659005943840951
F1 Score for NB with Chi2 and FR size = 255 is: 0.6525599508095921
F1 Score for NB with Chi2 and FR size = 383 is: 0.6419379001875415
F1 Score for NB with Chi2 and FR size = 511 is: 0.6325155020381459
F1 Score for NB with Chi2 and FR size = 767 is: 0.640569367377878
F1 Score for NB with Chi2 and FR size = 1023 is: 0.642050052708657
F1 Score for NB with Chi2 and FR size = 1535 is: 0.6202662504253172
F1 Score for NB with Chi2 and FR size = 2047 is: 0.6041890033779349
F1 Score for NB with Chi2 and FR size = 3071 is: 0.5845100198493624
F1 Score for NB with Chi2 and FR size = 4095 is: 0.5752755331504683

##############################################################
# SVM - real shitty. Prolly needs tuning. No idea what the parameters do.
##############################################################
F1 Score for SVM with Truncated SVD and FR size = 11 is: 0.41318577075098817
F1 Score for SVM with Truncated SVD and FR size = 15 is: 0.5175426312817617
F1 Score for SVM with Truncated SVD and FR size = 31 is: 0.6077687015078319
F1 Score for SVM with Truncated SVD and FR size = 47 is: 0.5850386135603527
F1 Score for SVM with Truncated SVD and FR size = 63 is: 0.5614429049211658
F1 Score for SVM with Truncated SVD and FR size = 95 is: 0.521521956304565
F1 Score for SVM with Truncated SVD and FR size = 127 is: 0.4564935064935065
F1 Score for SVM with Truncated SVD and FR size = 191 is: 0.3804329004329004

##############################################################
# KNN / IGR
##############################################################
F1 Score for KNN with IGR, K = 5, and FR size = 11 is: 0.6781559920379369
F1 Score for KNN with IGR, K = 5, and FR size = 15 is: 0.6889625030142271
F1 Score for KNN with IGR, K = 5, and FR size = 31 is: 0.6607035932652631
F1 Score for KNN with IGR, K = 5, and FR size = 47 is: 0.6151211361737677
F1 Score for KNN with IGR, K = 5, and FR size = 63 is: 0.5868787936873043
F1 Score for KNN with IGR, K = 5, and FR size = 95 is: 0.5525263096691668
F1 Score for KNN with IGR, K = 5, and FR size = 127 is: 0.5237440480737402
F1 Score for KNN with IGR, K = 5, and FR size = 191 is: 0.4898693935544814
F1 Score for KNN with IGR, K = 5, and FR size = 255 is: 0.48932454488386695
F1 Score for KNN with IGR, K = 5, and FR size = 383 is: 0.5016983170439392
F1 Score for KNN with IGR, K = 5, and FR size = 511 is: 0.46772313673384025
F1 Score for KNN with IGR, K = 5, and FR size = 767 is: 0.4947374364765669
F1 Score for KNN with IGR, K = 5, and FR size = 1023 is: 0.4185724250174378
F1 Score for KNN with IGR, K = 5, and FR size = 1535 is: 0.34073365231259967
F1 Score for KNN with IGR, K = 5, and FR size = 2047 is: 0.33022533022533024
F1 Score for KNN with IGR, K = 5, and FR size = 3071 is: 0.27794462847094425
F1 Score for KNN with IGR, K = 5, and FR size = 4095 is: 0.11124871001031991

##############################################################
# Additional KNN tuning
##############################################################
F1 Score for KNN with IGR, K = 3 and FR size = 11 is: 0.6816750536215146
F1 Score for KNN with IGR, K = 7 and FR size = 11 is: 0.6781559920379369
F1 Score for KNN with IGR, K = 15 and FR size = 11 is: 0.6781559920379369
F1 Score for KNN with IGR, K = 31 and FR size = 11 is: 0.6618596957416406
F1 Score for KNN with IGR, K = 63 and FR size = 11 is: 0.6104295544535425
F1 Score for KNN with IGR, K = 3 and FR size = 31 is: 0.649629826843449
F1 Score for KNN with IGR, K = 7 and FR size = 31 is: 0.6637805163421862
F1 Score for KNN with IGR, K = 15 and FR size = 31 is: 0.6637805163421862
F1 Score for KNN with IGR, K = 31 and FR size = 31 is: 0.6518296436854312
F1 Score for KNN with IGR, K = 63 and FR size = 31 is: 0.5231721611721611
F1 Score for KNN with IGR, K = 3 and FR size = 63 is: 0.5641839163578294
F1 Score for KNN with IGR, K = 7 and FR size = 63 is: 0.5908504248929781
F1 Score for KNN with IGR, K = 15 and FR size = 63 is: 0.5878474218899751
F1 Score for KNN with IGR, K = 31 and FR size = 63 is: 0.6242226579018184
F1 Score for KNN with IGR, K = 63 and FR size = 63 is: 0.4238152693987934
F1 Score for KNN with IGR, K = 3 and FR size = 127 is: 0.4997680097680098
F1 Score for KNN with IGR, K = 7 and FR size = 127 is: 0.5200779727095517
F1 Score for KNN with IGR, K = 15 and FR size = 127 is: 0.5694240542066629
F1 Score for KNN with IGR, K = 31 and FR size = 127 is: 0.4544733044733045
F1 Score for KNN with IGR, K = 63 and FR size = 127 is: 0.20292397660818712
F1 Score for KNN with IGR, K = 3 and FR size = 255 is: 0.4950388305981526
F1 Score for KNN with IGR, K = 7 and FR size = 255 is: 0.498414673046252
F1 Score for KNN with IGR, K = 15 and FR size = 255 is: 0.5200274797666101
F1 Score for KNN with IGR, K = 31 and FR size = 255 is: 0.31750572082379863
F1 Score for KNN with IGR, K = 63 and FR size = 255 is: 0.06797385620915032
F1 Score for KNN with IGR, K = 3 and FR size = 511 is: 0.4749159853391623
F1 Score for KNN with IGR, K = 7 and FR size = 511 is: 0.5318757763975155
F1 Score for KNN with IGR, K = 15 and FR size = 511 is: 0.3870117050356652
F1 Score for KNN with IGR, K = 31 and FR size = 511 is: 0.14327485380116958
F1 Score for KNN with IGR, K = 63 and FR size = 511 is: 0.0
F1 Score for KNN with IGR, K = 3 and FR size = 1023 is: 0.5012422360248447
F1 Score for KNN with IGR, K = 7 and FR size = 1023 is: 0.38614590416986433
F1 Score for KNN with IGR, K = 15 and FR size = 1023 is: 0.2514041514041514
F1 Score for KNN with IGR, K = 31 and FR size = 1023 is: 0.12235982112143103
F1 Score for KNN with IGR, K = 63 and FR size = 1023 is: 0.0

##############################################################
# Random Forest - depth = 10.
##############################################################
F1 Score for RF with Chi2 and FR size = 1 is: 0.6192673992673993
F1 Score for RF with Chi2 and FR size = 3 is: 0.6491444785799624
F1 Score for RF with Chi2 and FR size = 7 is: 0.6336105216622457
F1 Score for RF with Chi2 and FR size = 15 is: 0.6106502592859414
F1 Score for RF with Chi2 and FR size = 31 is: 0.6134057224379805
F1 Score for RF with Chi2 and FR size = 63 is: 0.6451215451215451
F1 Score for RF with Chi2 and FR size = 127 is: 0.6822017737340318
F1 Score for RF with Chi2 and FR size = 255 is: 0.6516450216450217
F1 Score for RF with Chi2 and FR size = 511 is: 0.634320420958352
F1 Score for RF with Chi2 and FR size = 1023 is: 0.6450521726383796
F1 Score for RF with Chi2 and FR size = 2047 is: 0.6897281880151735

Same - depth = 3
F1 Score for RF with Chi2 and FR size = 1 is: 0.6192673992673993
F1 Score for RF with Chi2 and FR size = 3 is: 0.6491444785799624
F1 Score for RF with Chi2 and FR size = 7 is: 0.64425390618939
F1 Score for RF with Chi2 and FR size = 15 is: 0.6803571428571429
F1 Score for RF with Chi2 and FR size = 31 is: 0.6599556238265916
F1 Score for RF with Chi2 and FR size = 63 is: 0.652459110946319
F1 Score for RF with Chi2 and FR size = 127 is: 0.6407548878137114
F1 Score for RF with Chi2 and FR size = 255 is: 0.6650281618023554
F1 Score for RF with Chi2 and FR size = 511 is: 0.6885724434111531
F1 Score for RF with Chi2 and FR size = 1023 is: 0.6662650335064128
F1 Score for RF with Chi2 and FR size = 2047 is: 0.6476532567049809

##############################################################
# Random Forest - tuning depth. FR = 127
##############################################################
F1 Score for RF with Chi2 and depth = 2 is: 0.6568747593747594
F1 Score for RF with Chi2 and depth = 3 is: 0.6824116743471582
F1 Score for RF with Chi2 and depth = 4 is: 0.6720471521942111
F1 Score for RF with Chi2 and depth = 5 is: 0.644966323998582
F1 Score for RF with Chi2 and depth = 6 is: 0.6548876549687909
F1 Score for RF with Chi2 and depth = 7 is: 0.6776446635734733
F1 Score for RF with Chi2 and depth = 8 is: 0.6222696601643971
F1 Score for RF with Chi2 and depth = 9 is: 0.6657781358325823
F1 Score for RF with Chi2 and depth = 11 is: 0.6161275252806693
F1 Score for RF with Chi2 and depth = 13 is: 0.6427564102564103